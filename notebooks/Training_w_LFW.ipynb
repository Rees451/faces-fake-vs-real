{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "XmpArFJ6BIt9",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Explanation\n",
    "Here I have tried to pre train a model using a synthetically generated dataset created using face-swappping on the [labeled faces in the wild (lfw)](http://vis-www.cs.umass.edu/lfw/) dataset. To create this supplementary dataset I used a modified version of the [FaceSwap app](https://github.com/MarekKowalski/FaceSwap) to perform random swaps between faces in the lfw dataset. The idea was to pretrain a model on this dataset and then refine the model to work with the orginal fake vs real dataset. As the fakes generated by FaceSwap were of much lower quality than in the fake vs real dataset my model was able to quite accurately categorise them. However it was not able to transfer this knowledge to the fake vs real dataset, implying that the underlying distributions are to distinct to enable transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "YzjOFuouBIuD"
   },
   "source": [
    "This notebook deals with training the neural network to classify images as photoshopped or not and is desgined to be run on google colab to make use of their free GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "O8lH7oVJvqWj"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-12-15T12:10:24.710429Z",
     "start_time": "2019-12-15T12:10:24.661145Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hcf9oAOmEOvR"
   },
   "outputs": [],
   "source": [
    "# Modules required\n",
    "import sys\n",
    "import os\n",
    "import importlib\n",
    "import joblib\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "JKQWHz2iolAW"
   },
   "source": [
    "## Colab setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "TuvXLhAxz5NA"
   },
   "source": [
    "Training neural nets on my laptop is very slow so I used Google Colab to speed things up a bit. This function gets run if I am using Colab which does some setup like downloading the dataset from github and linking my google drive so that model logs can get saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-12-15T11:48:26.755281Z",
     "start_time": "2019-12-15T11:48:26.739844Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "FmCgixjQ-jo1"
   },
   "outputs": [],
   "source": [
    "def colab_setup():\n",
    "\n",
    "    # Specify tensorflow version 2.0 and import, checking that gpu is used\n",
    "    %tensorflow_version 2.x\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Check that the GPU is being used\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "    # Add the project directory to path, allows access to items saved in google drive   \n",
    "    if project_dir not in sys.path:\n",
    "        sys.path.append(project_dir)\n",
    "\n",
    "    # If there is no data in this directory and on colab then copy the data\n",
    "    if not os.path.exists('./data'):\n",
    "\n",
    "        # Download raw data from github onto colab instance and move it into the raw data directory, utilises sparse checkout\n",
    "        !git init\n",
    "        !git config core.sparsecheckout true\n",
    "        !echo data >> .git/info/sparse-checkout\n",
    "        !git remote add origin -f https://github.com/ERees1/faces-fake-vs-real\n",
    "        !git pull origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "lOKx7kpior7j"
   },
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 54895,
     "status": "ok",
     "timestamp": 1579857580614,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "hgyIipR_or7k",
    "outputId": "46e03a43-583d-4e91-e3dd-4e92d43188d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "TensorFlow 2.x selected.\n",
      "Found GPU at: /device:GPU:0\n",
      "Initialized empty Git repository in /content/.git/\n",
      "Updating origin\n",
      "remote: Enumerating objects: 17, done.\u001b[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 18663 (delta 0), reused 14 (delta 0), pack-reused 18646\u001b[K\n",
      "Receiving objects: 100% (18663/18663), 468.52 MiB | 36.90 MiB/s, done.\n",
      "Resolving deltas: 100% (4186/4186), done.\n",
      "From https://github.com/ERees1/faces-fake-vs-real\n",
      " * [new branch]        master     -> origin/master\n",
      "From https://github.com/ERees1/faces-fake-vs-real\n",
      " * branch              master     -> FETCH_HEAD\n",
      "Checking out files: 100% (36106/36106), done.\n"
     ]
    }
   ],
   "source": [
    "# Add local drive to path if running on colab\n",
    "if 'edwardrees' in sys.exec_prefix:\n",
    "    device_loc = 'local'\n",
    "    project_dir = '..'\n",
    "    local_project_dir = project_dir\n",
    "else:\n",
    "    device_loc = 'colab'\n",
    "    local_project_dir = '.'\n",
    "\n",
    "    # Mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    project_dir = 'drive/My Drive/GA/Capstone/faces-fake-vs-real'\n",
    "\n",
    "    # Run setup function\n",
    "    colab_setup()\n",
    "\n",
    "# Want to be able to access files in my src folder\n",
    "sys.path.append(project_dir + '/src')\n",
    "\n",
    "# Import tensorflow, needs to be done after specifiying %tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "# Loading is one of my modules so need to import after linking my google drive\n",
    "import loading as ld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "m_eVv1V1v2r8"
   },
   "source": [
    "## Pretrain a model on LFW synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "ExecuteTime": {
     "end_time": "2019-12-15T12:07:09.579827Z",
     "start_time": "2019-12-15T11:54:55.850609Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Mpey575DBIun",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lfw_data_dir= './data/processed/LFW_faceswap_split'\n",
    "model_dir = project_dir+'/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "wRvuXjwgz5Ng"
   },
   "source": [
    "I initially utilised keras's `ImageDataGenerator` to load the data but when training I found it was much faster to use the `tf.data` api. As such I wrote a [class](../src/loading.py) to load the images and their corresponding labels in this way. I used some iamge augmentation in order to improve generalisability (and artifically increase the size of the training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "8LUGfsuBor75"
   },
   "outputs": [],
   "source": [
    "def train_model(model, model_id='', data_dir='', epochs=15,steps_per_epoch=None,\n",
    "                patience=5, lr=0.001):\n",
    "\n",
    "    # Get the input_shape the model requires\n",
    "    input_shape = model.input.get_shape().as_list()[-3:-1]\n",
    "    \n",
    "    # Load the data using DsLoader class\n",
    "    data = ld.DsLoader(data_dir, image_size=input_shape)\n",
    "    train = data.get_ds(split='train', augment=True, batch_size=32)\n",
    "    val = data.get_ds(split='val', augment=False, batch_size=32)\n",
    "\n",
    "    # Early stopping function\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                          verbose=1,\n",
    "                                          patience=patience)\n",
    "\n",
    "    # Setup a model checkpoint to save our best model\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        f'{model_dir}/{model_id}.h5',\n",
    "        monitor='val_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #  Fit the tensorflow model\n",
    "    model_fit = model.fit(train,\n",
    "                          epochs=epochs,\n",
    "                          steps_per_epoch=steps_per_epoch,\n",
    "                          validation_data=val,\n",
    "                          callbacks=[checkpoint])\n",
    "\n",
    "    # Save the model fit history\n",
    "    joblib.dump(model_fit.history, f'{model_dir}/{model_id}_history.gz')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "kwqnAFjgIv90"
   },
   "outputs": [],
   "source": [
    "# Function to construct the model, with various hyperparamters\n",
    "def build_model(img_width=64,\n",
    "                num_conv_blocks=3,\n",
    "                num_filters=32,\n",
    "                filter_size=3,\n",
    "                n_output_units=2,\n",
    "                dilation_rate=1):\n",
    "    \n",
    "    # Consituent model blocks\n",
    "    def conv_block(x, num_filters, filter_size, dilation_rate, block):\n",
    "        x = layers.Conv2D(num_filters,\n",
    "                          filter_size,\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          dilation_rate=dilation_rate,\n",
    "                          name=f'conv_{block}_0')(x)\n",
    "        x = layers.Conv2D(num_filters,\n",
    "                          filter_size,\n",
    "                          activation='relu',\n",
    "                          padding='same',\n",
    "                          dilation_rate=dilation_rate,\n",
    "                          name=f'conv_{block}_1')(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2), name=f'pool_{block}')(x)\n",
    "        x = layers.BatchNormalization(name=f'norm_{block}')(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def output_block(x, n_output_units):\n",
    "        x = layers.Flatten(name='output_flatten')(x)\n",
    "        x = layers.Dense(units=n_output_units, activation='softmax',\n",
    "                         name='output')(x)\n",
    "        return x\n",
    "\n",
    "    # Covert hparams to tuples where required\n",
    "    filter_size = (filter_size,) * 2\n",
    "    input_shape = (img_width,) * 2 + (3,)\n",
    "    dilation_rate = (dilation_rate, dilation_rate)\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape, name='input')\n",
    "    x = inputs\n",
    "    for i in range(num_conv_blocks):\n",
    "        x = conv_block(x, min(num_filters*2**i, 256), filter_size, dilation_rate, i)\n",
    "    x = output_block(x, n_output_units)\n",
    "\n",
    "    model = keras.Model(inputs, x, name='conv_1')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7343,
     "status": "ok",
     "timestamp": 1579857593657,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "K8qZM-DFor77",
    "outputId": "cdeab05d-11e0-4e57-9d87-974b81bf3f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 250, 250, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv_0_0 (Conv2D)            (None, 250, 250, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv_0_1 (Conv2D)            (None, 250, 250, 32)      9248      \n",
      "_________________________________________________________________\n",
      "pool_0 (MaxPooling2D)        (None, 125, 125, 32)      0         \n",
      "_________________________________________________________________\n",
      "norm_0 (BatchNormalization)  (None, 125, 125, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_1_0 (Conv2D)            (None, 125, 125, 64)      18496     \n",
      "_________________________________________________________________\n",
      "conv_1_1 (Conv2D)            (None, 125, 125, 64)      36928     \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 62, 62, 64)        0         \n",
      "_________________________________________________________________\n",
      "norm_1 (BatchNormalization)  (None, 62, 62, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_2_0 (Conv2D)            (None, 62, 62, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv_2_1 (Conv2D)            (None, 62, 62, 128)       147584    \n",
      "_________________________________________________________________\n",
      "pool_2 (MaxPooling2D)        (None, 31, 31, 128)       0         \n",
      "_________________________________________________________________\n",
      "norm_2 (BatchNormalization)  (None, 31, 31, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_3_0 (Conv2D)            (None, 31, 31, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv_3_1 (Conv2D)            (None, 31, 31, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool_3 (MaxPooling2D)        (None, 15, 15, 256)       0         \n",
      "_________________________________________________________________\n",
      "norm_3 (BatchNormalization)  (None, 15, 15, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_4_0 (Conv2D)            (None, 15, 15, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv_4_1 (Conv2D)            (None, 15, 15, 256)       590080    \n",
      "_________________________________________________________________\n",
      "pool_4 (MaxPooling2D)        (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "norm_4 (BatchNormalization)  (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_5_0 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv_5_1 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "pool_5 (MaxPooling2D)        (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "norm_5 (BatchNormalization)  (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "output_flatten (Flatten)     (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 4610      \n",
      "=================================================================\n",
      "Total params: 3,541,154\n",
      "Trainable params: 3,539,170\n",
      "Non-trainable params: 1,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn1_model = build_model(img_width=250, num_conv_blocks=6, num_filters=32, filter_size=3, dilation_rate=2)\n",
    "cnn1_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10385,
     "status": "ok",
     "timestamp": 1579857598186,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "kmHxEx8OMw1w",
    "outputId": "a7e9b215-d20a-479b-b8f1-2c71a2e40cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "cnn2_model = keras.Sequential([\n",
    "    base_model,\n",
    "    keras.layers.GlobalAvgPool2D(),\n",
    "    keras.layers.Dense(units=2, activation='softmax')\n",
    "                               \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 80523,
     "status": "ok",
     "timestamp": 1579857706670,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "yPxvapbY6iwD",
    "outputId": "3ca72590-c3af-4ea3-9f2f-8efd5e0bdd69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 50 steps, validate for 38 steps\n",
      "Epoch 1/10\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.8916\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.61913, saving model to drive/My Drive/GA/Capstone/faces-fake-vs-real/models/lfw_pretrain1.h5\n",
      "50/50 [==============================] - 29s 571ms/step - loss: 0.2558 - accuracy: 0.8919 - val_loss: 10.9478 - val_accuracy: 0.6191\n",
      "Epoch 2/10\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9668\n",
      "Epoch 00002: val_accuracy improved from 0.61913 to 0.69715, saving model to drive/My Drive/GA/Capstone/faces-fake-vs-real/models/lfw_pretrain1.h5\n",
      "50/50 [==============================] - 10s 208ms/step - loss: 0.1239 - accuracy: 0.9675 - val_loss: 5.6633 - val_accuracy: 0.6971\n",
      "Epoch 3/10\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0870 - accuracy: 0.9700\n",
      "Epoch 00003: val_accuracy improved from 0.69715 to 0.92534, saving model to drive/My Drive/GA/Capstone/faces-fake-vs-real/models/lfw_pretrain1.h5\n",
      "50/50 [==============================] - 11s 214ms/step - loss: 0.0866 - accuracy: 0.9706 - val_loss: 0.9110 - val_accuracy: 0.9253\n",
      "Epoch 4/10\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0473 - accuracy: 0.9860\n",
      "Epoch 00004: val_accuracy did not improve from 0.92534\n",
      "50/50 [==============================] - 10s 193ms/step - loss: 0.0469 - accuracy: 0.9862 - val_loss: 6.2386 - val_accuracy: 0.6871\n",
      "Epoch 5/10\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0824 - accuracy: 0.9739\n",
      "Epoch 00005: val_accuracy did not improve from 0.92534\n",
      "50/50 [==============================] - 9s 190ms/step - loss: 0.0813 - accuracy: 0.9737 - val_loss: 1.7534 - val_accuracy: 0.8339\n",
      "Epoch 6/10\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9864WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 500 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0585 - accuracy: 0.9864"
     ]
    }
   ],
   "source": [
    "history = train_model(cnn2_model, model_id='lfw_pretrain1', data_dir=lfw_data_dir, epochs=10, patience=4, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "dIP30y3Q_6Vi"
   },
   "outputs": [],
   "source": [
    "data = ld.DsLoader(lfw_data_dir, image_size=(224, 224))\n",
    "val = data.get_ds(split='val', augment=False, batch_size=32)\n",
    "# test = ld.load_test_generator(lfw_data_dir+'/test', img_shape=(224,224,3))\n",
    "test = data.get_ds(split='test', augment=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3175,
     "status": "ok",
     "timestamp": 1579858649576,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "IKcVSv2I3HUK",
    "outputId": "15c75373-7485-4710-99d4-d1842d4739b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 3s 71ms/step - loss: 2.0264 - accuracy: 0.8230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.026366394797438, 0.8229866]"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2_model.evaluate(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3389,
     "status": "ok",
     "timestamp": 1579858653713,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "mhbYfhBV3muX",
    "outputId": "98b62329-84a0-4aa6-aa15-c280202b8251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 3s 76ms/step - loss: 1.9886 - accuracy: 0.8106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.988599101963796, 0.8105616]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "vDAhA8OC7Eez"
   },
   "outputs": [],
   "source": [
    "fvr_data = ld.DsLoader('./data/processed/sf/all', image_size=(224, 224))\n",
    "fvr_train = fvr_data.get_ds(split='train', augment=False, batch_size=32)\n",
    "fvr_test = fvr_data.get_ds(split='test', augment=False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14172,
     "status": "ok",
     "timestamp": 1579859242605,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "Jk_hmWfT7uGD",
    "outputId": "3f964238-5810-4867-d4a9-fedc7a36f717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 14s 269ms/step - loss: 26.6204 - accuracy: 0.4767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[26.62042621537751, 0.47671568]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2_model.evaluate(fvr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "qIO_WCok85_2"
   },
   "outputs": [],
   "source": [
    "cnn2_model2 = keras.models.load_model(filepath=model_dir+'/lfw_pretrain1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64418,
     "status": "ok",
     "timestamp": 1579859911343,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "FIShAKSu9jUS",
    "outputId": "64cc07c0-c8ed-4d33-be9f-914da1018123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 51 steps, validate for 7 steps\n",
      "Epoch 1/5\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.8267 - accuracy: 0.5469\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.54412, saving model to drive/My Drive/GA/Capstone/faces-fake-vs-real/models/fvr_finetune.h5\n",
      "51/51 [==============================] - 28s 551ms/step - loss: 0.8220 - accuracy: 0.5502 - val_loss: 0.9708 - val_accuracy: 0.5441\n",
      "Epoch 2/5\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.6258 - accuracy: 0.6494\n",
      "Epoch 00002: val_accuracy improved from 0.54412 to 0.59804, saving model to drive/My Drive/GA/Capstone/faces-fake-vs-real/models/fvr_finetune.h5\n",
      "51/51 [==============================] - 9s 185ms/step - loss: 0.6253 - accuracy: 0.6483 - val_loss: 1.2539 - val_accuracy: 0.5980\n",
      "Epoch 3/5\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.6906\n",
      "Epoch 00003: val_accuracy did not improve from 0.59804\n",
      "51/51 [==============================] - 9s 170ms/step - loss: 0.5789 - accuracy: 0.6893 - val_loss: 3.4950 - val_accuracy: 0.5196\n",
      "Epoch 4/5\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.5546 - accuracy: 0.7188\n",
      "Epoch 00004: val_accuracy did not improve from 0.59804\n",
      "51/51 [==============================] - 9s 170ms/step - loss: 0.5514 - accuracy: 0.7200 - val_loss: 1.1850 - val_accuracy: 0.5098\n",
      "Epoch 5/5\n",
      "50/51 [============================>.] - ETA: 0s - loss: 0.5309 - accuracy: 0.7344\n",
      "Epoch 00005: val_accuracy did not improve from 0.59804\n",
      "51/51 [==============================] - 9s 171ms/step - loss: 0.5298 - accuracy: 0.7359 - val_loss: 1.4318 - val_accuracy: 0.5245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f18575c3ba8>"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_model(cnn2_model2,'fvr_finetune' ,'./data/processed/sf/all', epochs=5, lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2275,
     "status": "ok",
     "timestamp": 1579860055816,
     "user": {
      "displayName": "Edward Rees",
      "photoUrl": "",
      "userId": "13022692274990714437"
     },
     "user_tz": 0
    },
    "id": "LIqs4X1O_gzK",
    "outputId": "f5d1ae4d-fccf-4db9-d23c-f624fce555f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 267ms/step - loss: 1.2678 - accuracy: 0.5415\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.267842514174325, 0.54146343]"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn2_model2.evaluate(fvr_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training_w_LFW.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "410.8088073730469px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "540.5147094726562px",
    "left": "1633.4742431640625px",
    "right": "20px",
    "top": "120px",
    "width": "400.64337158203125px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
